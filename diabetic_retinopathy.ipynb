import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# import seaborn as sns

from google.colab import files
uploaded = files.upload()

import io
df = pd.read_csv(io.BytesIO(uploaded['messidor_features.csv']))
# Dataset is now stored in a Pandas Dataframe

df.head(n=6)

df.describe()

np.shape(df)

df.isnull().sum().sum()

df['0'].value_counts()


data = df.drop(['class'], axis=1)

target = df["class"]

#Train Test Split
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
X_train, X_test, y_train, y_test = train_test_split(data, df["class"], test_size=0.15, random_state=42)


#Naive Bayes - 1
from sklearn.naive_bayes import GaussianNB
model = GaussianNB()
model.fit(X_train, y_train)
predicted_NB = model.predict(X_test)


accuracy_score(y_test, predicted_NB)


#KNN - 2
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=6)
model.fit(X_train, y_train)
predicted_KNN = model.predict(X_test)

accuracy_score(y_test, predicted_KNN)


#Random Forest - 3
from sklearn.ensemble import RandomForestClassifier
model= RandomForestClassifier()
model.fit(X_train, y_train)
predicted_RF = model.predict(X_test)

importances = model.feature_importances_
std = np.std([tree.feature_importances_ for tree in model.estimators_],
             axis=0)
indices = np.argsort(importances)[::-1]

accuracy_score(y_test, predicted_RF)


# Print the feature ranking
print("Feature ranking:")

for f in range(X_train.shape[1]):
    print("%d. feature %d (%f)" % (f + 1, indices[f], importances[indices[f]]))
    
    
#Support Vector Machines - 4
from sklearn import svm
model = svm.SVC()
model.fit(X_train, y_train)
predicted_SVM = model.predict(X_test)


accuracy_score(y_test, predicted_SVM)


#Decision Tree - 5
from sklearn import tree
model = tree.DecisionTreeClassifier(criterion='gini')
model.fit(X_train, y_train)
predicted_dt = model.predict(X_test)

accuracy_score(y_test, predicted_dt)


#Adaboost - 6
from sklearn.ensemble import AdaBoostClassifier
model = AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=1),
                         algorithm="SAMME",
                         n_estimators=200)

model.fit(X_train, y_train)
predicted_ADB = model.predict(X_test)


accuracy_score(y_test, predicted_ADB)



#Gradient Boosting Machine - 7
from sklearn.ensemble import GradientBoostingClassifier
model= GradientBoostingClassifier(n_estimators=100, learning_rate=0.05, max_depth=1, random_state=0)
model.fit(X_train, y_train)
predicted_GBM = model.predict(X_test)


accuracy_score(y_test, predicted_GBM)


from sklearn.preprocessing import StandardScaler  
scaler = StandardScaler() 

scaler.fit(X_train)  
X_train = scaler.transform(X_train)  
X_test = scaler.transform(X_test)


#Multilayer Perceptron - 8
from sklearn.neural_network import MLPClassifier
model = MLPClassifier(solver='lbfgs', alpha=1e-5,
                     hidden_layer_sizes=(5, 2), random_state=1)

model.fit(X_train, y_train)
predicted_MLP = model.predict(X_test)

accuracy_score(y_test, predicted_MLP)


